# llm-fact-check

# Welcome to LLM Fact Checker! ðŸš€ðŸ¤–

Hi there,  our main goal is to ensure that we provide accurate and reliable information to our users. To achieve this, we have implemented a robust system that leverages the capabilities of Language Model (LLM) and LangChain Agents.

When we encounter claims or statements, our process involves creating a set of assumptions regarding their truthfulness or falsehood. We understand that assumptions can guide our investigation and help us uncover the veracity of the claims.

To validate these assumptions, we employ various trusted sources such as Google Fact Check, Wikipedia, the Knowledge Graph, and perform comprehensive searches using Google's search engine. These resources enable us to gather relevant and factual information to support or debunk the assumptions made.

In addition to these automated processes, we recognize the importance of human validation. In cases where LLM requires validation or where the information is complex and requires human judgment, we include human validators. These validators play a crucial role in ensuring the accuracy and integrity of the information provided.

By combining the power of LLM, LangChain Agents, and the expertise of human validators, we strive to deliver the most reliable and verified information to our users. Our commitment is to continuously improve our fact-checking methods and enhance the overall user experience.


How to Start 

Install Dependency by pipenv install 

Run App by chainlit run fact_check_llm.py 

