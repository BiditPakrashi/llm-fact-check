{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.agents import tool\n",
    "import factapidoc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get your [OpenAI API Key](https://platform.openai.com/account/api-keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def check_claim_chain(claim) -> str:\n",
    "    \"\"\"  This tool be used as first option and  to break claims in  multiple assumtions \\\n",
    "    Returns  Statement is final statement after fact check  \\  \n",
    "        The input should always be  claim as string, \\\n",
    "        and this function will always looks into the claim then  \\\n",
    "    Make a bullet point list of the assumptions \\\n",
    "        Take every assumtion and do fact check\\\n",
    "        fact check will be outside this function.\n",
    "        Fact check will be done by other tools\"\"\"\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    template = \"\"\"{question}\\n\\n\"\"\"\n",
    "    prompt_template = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "    question_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    template = \"\"\"Here is a statement:\n",
    "    {statement}\n",
    "    Make a bullet point list of the assumptions you made when producing the above statement.\\n\\n\"\"\"\n",
    "    prompt_template = PromptTemplate(input_variables=[\"statement\"], template=template)\n",
    "    assumptions_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    template = \"\"\"Here is a bullet point list of assertions:\n",
    "    {assertions}\n",
    "    For each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n\"\"\"\n",
    "    prompt_template = PromptTemplate(input_variables=[\"assertions\"], template=template)\n",
    "    fact_checker_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    template = f\"\"\"In light of the above facts, how would you answer the question '{claim}'\"\"\"\n",
    "    template = \"\"\"{facts}\\n\"\"\" + template\n",
    "    prompt_template = PromptTemplate(input_variables=[\"facts\"], template=template)\n",
    "    answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "    overall_chain = SimpleSequentialChain(chains=[question_chain, assumptions_chain, fact_checker_chain, answer_chain], verbose=True)\n",
    "    \n",
    "    return overall_chain.run(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def check_fact_google(statement: str) -> str:\n",
    "    \"\"\"Returns If the  statement given as parameter is not empty \n",
    "    and returns true or false or partly false using  factchain. \n",
    "    in case of partly false as Human\n",
    "    If APIChain response is Empty then use other tool \"\"\"\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    api_key = os.environ['API_KEY']\n",
    "    factchain = APIChain.from_llm_and_api_docs(llm,factapidoc.GOOGLEFACT_DOCS, verbose=True)\n",
    "    return factchain.run(f\"Check if it is true for {statement} use  api_key like {api_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "tools = load_tools(['llm-math',\"wikipedia\",\"human\"], llm=llm)\n",
    "\n",
    "tools = tools + [check_claim_chain , check_fact_google ,PythonREPLTool(),googleetool] \n",
    "\n",
    "print(type((tools)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "   tools , \n",
    "    llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"A massive section of I-95 in Philadelphia has collapsed after a huge tanker caught fire. This is a logistical nightmare that will take a long time to fix and affect a lot of daily commuters.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
